[RGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
; in_dim = 64
; hidden_dim = 64
in_dim = 64
hidden_dim = 64
# number of weight matrix bases
n_bases = 40
; num_layers = 3
num_layers = 3

; max_epoch = 50
max_epoch = 200
patience = 50
batch_size = 128
fanout = 4

validation = True
use_self_loop = False
mini_batch_flag = False
use_uva = True


[CompGCN]
learning_rate = 0.01
weight_decay = 0.0001
dropout = 0.2

seed = 0
; num_layers = 2
num_layers = 2
; in_dim = 32
in_dim = 32
; hidden_dim = 32
hidden_dim = 32
; out_dim = 32
out_dim = 32
;We restrict the number of hidden units to 32. from paper

max_epoch = 500
; max_epoch = 100
patience = 100
;sub(subtraction) mult(multiplication) ccorr(circular-correlation)
comp_fn = sub
validation = True
mini_batch_flag = False
batch_size = 128
fanout = 4


[HAN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6

; hidden_dim = 128
; out_dim = 16
hidden_dim = 128
out_dim = 16
; number of attention heads
num_heads = 8
max_epoch = 200
patience = 100
mini_batch_flag = False


[HGT]
seed = 0
learning_rate = 0.001
weight_decay = 0.0001
dropout = 0.4

batch_size = 5120
patience = 40
; hidden_dim = 64
hidden_dim = 64
; out_dim = 16
out_dim = 16
; num_layers = 2
num_layers = 2
num_heads = 8
num_workers = 64
max_epoch = 500
mini_batch_flag = False
fanout = 5
norm = True
use_uva = True


[HPN]
seed = 0
learning_rate = 0.005
weight_decay = 0.001
dropout = 0.6
; k_layer = 2
k_layer = 2
alpha = 0.1
edge_drop = 0

; hidden_dim = 64
; out_dim = 16
hidden_dim = 64
out_dim = 16
max_epoch = 200
patience = 100
mini_batch_flag = False


[SimpleHGN]
; hidden_dim = 256
hidden_dim = 256
; num_layers = 3
num_layers = 2
num_heads = 8
feats_drop_rate = 0.2
slope = 0.05
edge_dim = 64
seed = 0
max_epoch = 500
; max_epoch = 6
patience = 100
lr = 0.001
weight_decay = 5e-4
beta = 0.05
residual = True
mini_batch_flag = False
fanout = 5
batch_size = 2048
use_uva = True


[HetSANN]
lr = 0.0001
; lr = 0.001
weight_decay = 0.0005
dropout = 0.2
seed = 0
; hidden_dim = 64
hidden_dim = 256
; num_layers = 2
num_layers = 2
num_heads = 16
max_epoch = 10000
patience = 100
slope = 0.2
residual = True
mini_batch_flag = False
batch_size = 2048
fanout = 5
use_uva = True

[ieHGCN]
num_layers = 5
; num_layers = 5
hidden_dim = 256
; hidden_dim = 64
attn_dim = 32
; out_dim = 16
out_dim = 256
patience = 100
seed = 0
lr = 0.001
weight_decay = 5e-4
max_epoch = 3500
mini_batch_flag = False
fanout = 10
batch_size = 512
dropout = 0.2
bias = True
batchnorm = True


[RGAT]
;Input tensor dimension
; in_dim = 64
;The number of layers
; num_layers = 3
num_layers = 3
;The dimension of hidden layers tensor
; hidden_dim = 64
in_dim = 64
hidden_dim = 64
patience = 100
seed = 0
lr = 0.01
weight_decay = 5e-4
max_epoch = 350
dropout = 0.2
;The number of attention heads
num_heads = 3
;Output tensor dimension
; out_dim = 3
out_dim = 16